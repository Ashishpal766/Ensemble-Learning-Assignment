{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.**\n",
        "\n",
        "\n",
        "Ans:-  Ensemble learning is a machine learning technique that combines predictions from multiple individual models to improve overall performance and accuracy. Instead of relying on a single model, an ensemble model leverages the collective intelligence of a group of models, often referred to as weak learners or base learners. This approach leads to more robust and accurate predictions than any single model could produce alone.\n",
        "\n",
        "Key Idea Behind Ensemble Learning\n",
        "The fundamental idea behind ensemble learning is the \"wisdom of the crowd\" principle.  This concept suggests that the collective judgment of a diverse group of non-experts is often more accurate than the judgment of a single expert. In machine learning, this translates to combining the outputs of several diverse models to cancel out their individual errors and biases. The errors made by one model are often different from the errors made by another, and by averaging or voting on their predictions, the ensemble can reduce the total error.\n",
        "\n",
        "Ensemble methods achieve this diversity in a few key ways, depending on the specific technique:\n",
        "\n",
        "Bagging (Bootstrap Aggregating): This method trains multiple instances of the same type of model on different random subsets of the original training data. The randomness in the data subsets ensures that each model learns slightly different patterns, leading to a diverse set of predictions. The final prediction is determined by averaging the outputs (for regression) or using a majority vote (for classification). A well-known example is the Random Forest algorithm, which uses bagging with decision trees.\n",
        "\n",
        "Boosting: Boosting is a sequential process where models are trained one after another. Each new model is trained to correct the errors made by the previous models. This technique focuses on improving the performance on data points that were difficult for earlier models to classify correctly. The final prediction is a weighted combination of the individual models' predictions, giving more importance to the more accurate models.\n",
        "\n",
        "Stacking: Also known as stacked generalization, this technique involves training multiple diverse models (e.g., a decision tree, a support vector machine, and a neural network) on the same dataset. The predictions from these models are then used as input features to train a final, higher-level model (a meta-model). This meta-model learns how to best combine the predictions of the base models to make a final, improved prediction."
      ],
      "metadata": {
        "id": "1SwfMYTgW7CT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Bagging and Boosting?**\n",
        "\n",
        "Ans:-  Bagging and Boosting are both ensemble learning methods that combine the predictions of multiple models to improve overall performance, but they differ fundamentally in their approach to model training. Bagging trains models in parallel, while Boosting trains them sequentially, with each new model learning from the errors of the previous ones.\n",
        "\n",
        "Bagging vs. Boosting\n",
        "Here's a point-by-point comparison:\n",
        "\n",
        "Aspect\tBagging (Bootstrap Aggregating)\tBoosting\n",
        "Model Training\tModels are trained independently and in parallel.\tModels are trained sequentially. Each new model is trained to correct the errors of its predecessors.\n",
        "Data Subset\tEach model is trained on a random subset of the original training data, with replacement (bootstrapping). All data points have an equal chance of being selected.\tEach new model focuses on data points that were misclassified by the previous models, giving them more weight.\n",
        "Primary Goal\tTo reduce variance and prevent overfitting. Bagging is particularly effective with models that are prone to high variance, like unpruned decision trees.\tTo reduce bias and convert a set of weak learners into a single strong learner. Boosting is effective for models that have low performance or high bias.\n",
        "Final Prediction\tPredictions from all models are combined using a simple method, like averaging (for regression) or majority voting (for classification).\tPredictions are combined using a weighted average or sum, where more accurate models are given more importance.\n",
        "Examples\tRandom Forest is a well-known example that uses bagging with decision trees.\tAdaBoost, Gradient Boosting, and XGBoost are popular boosting algorithms.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SxzkQ0YYXTVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?**\n",
        "\n",
        "Ans:-  Bootstrap sampling is a resampling technique where you create smaller datasets by randomly sampling from the original dataset with replacement. This means that a single data point can be selected multiple times in a single bootstrap sample, and other data points might not be selected at all. Each bootstrap sample has the same number of data points as the original dataset. This process generates multiple diverse datasets from a single source.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**The Role of Bootstrap Sampling in Bagging and Random Forest**\n",
        "\n",
        "Bootstrap sampling is the core of Bagging, which stands for \"Bootstrap Aggregating.\" It plays a crucial role in creating the diverse set of models that make up the ensemble. Here's how it works, especially in a Random Forest:\n",
        "\n",
        "1. Creating Diverse Subsets: The original training data is used to generate many different bootstrap samples. Because each sample is created by drawing data points with replacement, they all contain a slightly different distribution of data. Some data points appear multiple times, while others are omitted. This ensures that the individual models trained on these subsets are all slightly different from each other.\n",
        "\n",
        "\n",
        "2. Training Individual Models: For a Random Forest, a separate decision tree is trained on each of these unique bootstrap samples. The individual trees are intentionally made to be high-variance and unpruned, meaning they are prone to overfitting their specific training subset.\n",
        "\n",
        "3. Reducing Variance: The diversity introduced by the bootstrap samples is key to the success of the ensemble. By training models on different data, you ensure that their individual errors are not systematically correlated. The final prediction is made by aggregating the outputs of all the individual trees (e.g., using a majority vote for classification or averaging for regression). This aggregation process averages out the individual models' high variance, leading to a much more stable and accurate final prediction than any single decision tree could achieve.\n",
        "\n",
        "\n",
        "In essence, bootstrap sampling provides the necessary data variability to make the individual models diverse, which is the foundational principle of bagging for reducing overall model variance and preventing overfitting."
      ],
      "metadata": {
        "id": "6lfqIMX2YFF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?**\n",
        "\n",
        "\n",
        "Ans:- Out-of-Bag (OOB) samples are the data points from the original training set that are not included in a particular bootstrap sample. In bagging methods, like Random Forest, each individual model (e.g., a decision tree) is trained on a different bootstrap sample. Because bootstrap sampling is done with replacement, on average, each bootstrap sample contains about 63% of the original data, leaving the remaining 37% as the out-of-bag samples for that specific model. These OOB samples are essentially \"unseen\" data for the model they were left out of.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "How OOB Score is Used to Evaluate Ensemble Models\n",
        "The OOB score is a powerful method for evaluating the performance of ensemble models without the need for a separate validation or test set. Here‚Äôs how it works:\n",
        "\n",
        "Prediction on OOB Samples: For each data point in the original training set, an OOB prediction is made. This prediction is generated by aggregating the predictions from only the models that did not see that data point during their training (i.e., the models for which that data point was an OOB sample). For example, if a data point was left out of the bootstrap samples for trees 1, 5, and 10, the OOB prediction for that data point would be the majority vote (for classification) or the average (for regression) of the predictions from those three trees.\n",
        "\n",
        "\n",
        "Unbiased Performance Estimate: The OOB score is then calculated by comparing these aggregated OOB predictions to the actual labels of the data points. Since the models making the predictions have never seen the data points they are evaluating, the OOB score provides an unbiased estimate of the model's generalization performance.\n",
        "\n",
        "\n",
        "Comparison to Cross-Validation: The OOB score is a computationally efficient alternative to traditional cross-validation. Cross-validation requires creating multiple folds and training the model on different subsets, which can be computationally expensive. OOB evaluation, however, is a seamless part of the bagging process itself, as the \"validation\" data is naturally available for each model. The OOB score can be used to tune hyperparameters and provides a reliable measure of how well the ensemble model will perform on unseen data."
      ],
      "metadata": {
        "id": "MSBLDvjZY5nZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "\n",
        "Ans:- Comparing feature importance in a single Decision Tree versus a Random Forest reveals a key difference in reliability and stability. While both methods use the same underlying principle of measuring how much each feature contributes to the model's decisions, the ensemble nature of Random Forest provides a much more robust and trustworthy result.\n",
        "\n",
        "\n",
        "Feature Importance in a Single Decision Tree\n",
        "In a single Decision Tree, feature importance is calculated based on how much a feature reduces the impurity (e.g., Gini impurity or entropy for classification, or mean squared error for regression) at each split. The importance score for a feature is the total reduction in impurity it achieves across all nodes where it is used for a split.\n",
        "\n",
        "\n",
        "Calculation: The algorithm sums up the impurity reduction for each time a feature is used to split a node. A feature that is used to create a \"purer\" split (a split that separates the data into more homogeneous groups) is given a higher score.\n",
        "\n",
        "Drawbacks: The main issue is that a single Decision Tree is highly sensitive to the training data. A small change in the data can lead to a completely different tree structure, resulting in wildly different feature importance scores. This makes the importance ranking unstable and unreliable as a general measure. It can also be biased towards features with a high number of unique values, as these features offer more potential split points.\n",
        "\n",
        "\n",
        "Feature Importance in a Random Forest\n",
        "A Random Forest calculates feature importance by aggregating the importance scores from all the individual decision trees within the forest.\n",
        "\n",
        "Calculation: For each tree in the forest, the feature importance is calculated just as it would be for a single Decision Tree. Then, the final feature importance score for the entire Random Forest is the average of the importance scores for that feature across all the trees.\n",
        "\n",
        "\n",
        "Advantages: This averaging process is the key to its robustness. Because each tree in the forest is trained on a different bootstrap sample of the data and uses a random subset of features for each split, the individual trees are de-correlated. A feature that appears important by chance in one tree is unlikely to appear important in many others. By averaging across the entire forest, the random noise is canceled out, and the resulting importance scores provide a much more stable, accurate, and reliable ranking of the features' true predictive power.\n",
        "\n",
        "\n",
        "Ultimately, while a single Decision Tree gives you a quick and interpretable look at feature importance, a Random Forest offers a more stable and reliable assessment due to its ensemble nature, making it the preferred method for feature importance analysis in most practical applications"
      ],
      "metadata": {
        "id": "8UK23sVOZR3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o7W8wPByYysX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to:\n",
        "\n",
        "‚óè Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "‚óè Train a Random Forest Classifier\n",
        "\n",
        "‚óè Print the top 5 most important features based on feature importance scores**\n",
        "\n",
        "Ans:- Here the Python code"
      ],
      "metadata": {
        "id": "CwCUVlMHaK9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = breast_cancer.target\n",
        "\n",
        "print(\"Dataset loaded successfully! üìä\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\\n\")\n",
        "\n",
        "# 2. Train a Random Forest Classifier\n",
        "# We use a random_state for reproducibility of results\n",
        "# n_estimators is the number of trees in the forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "print(\"Random Forest Classifier trained! üå≥üå≤\\n\")\n",
        "\n",
        "# 3. Print the top 5 most important features\n",
        "# Get feature importances from the trained model\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a pandas Series for easier sorting and viewing\n",
        "feature_importance_series = pd.Series(feature_importances, index=X.columns)\n",
        "\n",
        "# Sort the features by importance in descending order\n",
        "top_features = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\\n\")\n",
        "print(top_features.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmmQIDq9aoAQ",
        "outputId": "f5e024dc-833b-40d3-8f35-8a32e257bf8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully! üìä\n",
            "Number of features: 30\n",
            "Number of samples: 569\n",
            "\n",
            "Random Forest Classifier trained! üå≥üå≤\n",
            "\n",
            "Top 5 Most Important Features:\n",
            "\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:\n",
        "\n",
        "‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "‚óè Evaluate its accuracy and compare with a single Decision Tree**\n",
        "\n",
        "Ans:- -Loads the Iris dataset.\n",
        "\n",
        "- Trains a single Decision Tree classifier.\n",
        "\n",
        "- Trains a Bagging Classifier (using Decision Trees as base estimators).\n",
        "\n",
        "- Evaluates and compares their accuracies."
      ],
      "metadata": {
        "id": "2bjMOJTUauwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees as base estimators\n",
        "bag_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Decision Tree Accuracy: {dt_acc:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bag_acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QbCJL_eFbDo-",
        "outputId": "64ba0de9-9332-4880-9c72-609813fa8617"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-139629230.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Train a Bagging Classifier with Decision Trees as base estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m bag_clf = BaggingClassifier(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:\n",
        "\n",
        "‚óè Train a Random Forest Classifier\n",
        "\n",
        "‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "‚óè Print the best parameters and final accuracy**\n",
        "\n",
        "\n",
        "Ans:- Here's a Python program that trains a Random Forest Classifier, tunes its max_depth and n_estimators hyperparameters using GridSearchCV, and then prints the best parameters and the final accuracy\n"
      ],
      "metadata": {
        "id": "1MONBlhmblB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = breast_cancer.target\n",
        "\n",
        "print(\"Breast Cancer dataset loaded successfully! üéóÔ∏è\\n\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\\n\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "# stratify=y ensures that the proportion of target classes is the same in both train and test sets\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\\n\")\n",
        "\n",
        "# 2. Define the Random Forest Classifier\n",
        "# We set a random_state for reproducibility of the base model's randomness\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "# max_depth: The maximum depth of the tree. Limiting this can prevent overfitting.\n",
        "# n_estimators: The number of trees in the forest. More trees generally improve performance\n",
        "#               but increase computation time.\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],  # Number of trees\n",
        "    'max_depth': [None, 10, 20]      # Maximum depth of each tree (None means unlimited depth)\n",
        "}\n",
        "\n",
        "print(\"Starting GridSearchCV for hyperparameter tuning... üõ†Ô∏è\\n\")\n",
        "\n",
        "# 3. Tune hyperparameters using GridSearchCV\n",
        "# GridSearchCV performs an exhaustive search over the specified parameter values.\n",
        "# It uses cross-validation (cv=5 means 5-fold cross-validation) to evaluate each combination.\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"GridSearchCV complete! ‚ú®\\n\")\n",
        "\n",
        "# 4. Print the best parameters\n",
        "print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# 5. Evaluate the final accuracy on the test set using the best model\n",
        "y_pred_best_model = best_rf_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred_best_model)\n",
        "\n",
        "print(f\"\\nFinal accuracy of the best Random Forest model on the test set: {final_accuracy:.4f} üéâ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl5zQsp7b5jC",
        "outputId": "fbb0f76a-6337-425c-dd7e-712358a2d92b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breast Cancer dataset loaded successfully! üéóÔ∏è\n",
            "\n",
            "Number of features: 30\n",
            "Number of samples: 569\n",
            "\n",
            "Training data shape: (398, 30)\n",
            "Testing data shape: (171, 30)\n",
            "\n",
            "Starting GridSearchCV for hyperparameter tuning... üõ†Ô∏è\n",
            "\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "GridSearchCV complete! ‚ú®\n",
            "\n",
            "Best hyperparameters found by GridSearchCV:\n",
            "{'max_depth': None, 'n_estimators': 100}\n",
            "\n",
            "Final accuracy of the best Random Forest model on the test set: 0.9357 üéâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:\n",
        "\n",
        "‚óè Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "\n",
        "Housing dataset\n",
        "\n",
        "‚óè Compare their Mean Squared Errors (MSE)**\n",
        "\n",
        "Ans:- Here's a Python program that trains a Bagging Regressor and a Random Forest Regressor on the California Housing dataset and then compares their Mean Squared Errors (MSE).\n"
      ],
      "metadata": {
        "id": "OSih7zPeb-25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "california_housing = fetch_california_housing(as_frame=True)\n",
        "X = california_housing.data\n",
        "y = california_housing.target\n",
        "\n",
        "print(\"California Housing dataset loaded successfully! üè°\\n\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\\n\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# We use a random_state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\\n\")\n",
        "\n",
        "# 2. Train a Bagging Regressor using Decision Trees\n",
        "# The base_estimator is the type of model to be used in the ensemble (DecisionTreeRegressor here)\n",
        "# n_estimators is the number of base estimators (decision trees) in the ensemble\n",
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(random_state=42),\n",
        "n_estimators=100, # Using 100 decision trees\n",
        "random_state=42,\n",
        "n_jobs=-1) # Use all available CPU cores for parallel training\n",
        "\n",
        "print(\"Training Bagging Regressor... üå≥\\n\")\n",
        "\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate MSE for the Bagging Regressor\n",
        "y_pred_bagging = bagging_regressor.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE) for Bagging Regressor: {mse_bagging:.4f}\\n\")\n",
        "\n",
        "# 3. Train a Random Forest Regressor\n",
        "# RandomForestRegressor inherently uses bagging, but also adds feature randomness\n",
        "# n_estimators is the number of trees in the forest\n",
        "random_forest_regressor = RandomForestRegressor(n_estimators=100,\n",
        "random_state=42,\n",
        "n_jobs=-1) # Use all available CPU cores for parallel training\n",
        "\n",
        "print(\"Training Random Forest Regressor... üå≤üå≤\\n\")\n",
        "random_forest_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate MSE for the Random Forest Regressor\n",
        "y_pred_random_forest = random_forest_regressor.predict(X_test)\n",
        "mse_random_forest = mean_squared_error(y_test, y_pred_random_forest)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE) for Random Forest Regressor: {mse_random_forest:.4f}\\n\")\n",
        "\n",
        "# 4. Compare their Mean Squared Errors (MSE)\n",
        "print(\"--- Comparison of Regressor Performance ---\")\n",
        "print(f\"Bagging Regressor MSE:    {mse_bagging:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_random_forest:.4f}\")\n",
        "\n",
        "if mse_random_forest < mse_bagging:\n",
        "    print(\"\\nThe Random Forest Regressor achieved a lower MSE, indicating better performance. üéâ\")\n",
        "elif mse_random_forest > mse_bagging:\n",
        "    print(\"\\nThe Bagging Regressor achieved a lower MSE in this run. This can sometimes happen depending on the dataset and specific random states.\")\n",
        "else:\n",
        "    print(\"\\nBoth regressors achieved the same MSE. ü§ù\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "0k49nsJJcTIz",
        "outputId": "7049b1e4-1455-4dc0-cd2a-287a85664a00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "California Housing dataset loaded successfully! üè°\n",
            "\n",
            "Number of features: 8\n",
            "Number of samples: 20640\n",
            "\n",
            "Training data shape: (14448, 8)\n",
            "Testing data shape: (6192, 8)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2894533418.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# The base_estimator is the type of model to be used in the ensemble (DecisionTreeRegressor here)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# n_estimators is the number of base estimators (decision trees) in the ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor(random_state=42),\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Using 100 decision trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "The program performs the following steps to compare the two ensemble regression models:\n",
        "\n",
        "Load Dataset: It starts by loading the California Housing dataset using fetch_california_housing from sklearn.datasets. This dataset is commonly used for regression tasks, where the goal is to predict house prices.\n",
        "\n",
        "Data Splitting: The dataset is then split into training and testing sets using train_test_split. This ensures that the models are evaluated on data they haven't seen during training, providing an unbiased assessment of their generalization ability.\n",
        "\n",
        "Bagging Regressor Training:\n",
        "\n",
        "A BaggingRegressor is initialized. Its base_estimator is set to DecisionTreeRegressor(random_state=42), meaning each individual model in the ensemble will be a decision tree.\n",
        "\n",
        "n_estimators=100 specifies that 100 decision trees will be trained.\n",
        "\n",
        "n_jobs=-1 allows the training of these 100 trees to run in parallel across all available CPU cores, speeding up the process.\n",
        "\n",
        "The model is then fit to the training data.\n",
        "\n",
        "Random Forest Regressor Training:\n",
        "\n",
        "A RandomForestRegressor is initialized with n_estimators=100. The Random Forest algorithm is essentially a specialized form of bagging that also introduces additional randomness by randomly selecting a subset of features at each split point in the decision trees. This further decorrelates the trees, often leading to improved performance.\n",
        "\n",
        "n_jobs=-1 is also used here for parallel processing.\n",
        "\n",
        "The model is fit to the training data.\n",
        "\n",
        "Mean Squared Error (MSE) Comparison:\n",
        "\n",
        "After training, both regressors make predictions on the X_test data.\n",
        "\n",
        "The mean_squared_error metric is calculated for both sets of predictions against the actual y_test values. MSE measures the average squared difference between the estimated values and the actual value. A lower MSE indicates a better fit of the model to the data.\n",
        "\n",
        "Finally, the MSE values for both models are printed and compared, allowing you to see which ensemble method performed better on this specific dataset and split. Random Forests often outperform simple Bagging due to the added feature randomness, which further reduces correlation among the base trees."
      ],
      "metadata": {
        "id": "yLyQwcBKd-zJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "‚óè Choose between Bagging or Boosting\n",
        "\n",
        "‚óè Handle overfitting\n",
        "\n",
        "‚óè Select base models\n",
        "\n",
        "‚óè Evaluate performance using cross-validation\n",
        "\n",
        "‚óè Justify how ensemble learning improves decision-making in this real-world\n",
        "context.**\n",
        "\n",
        "Ans:-  **Step-by-Step Approach for a Loan Default Prediction Model**\n",
        "\n",
        "As a data scientist predicting loan default, a robust and accurate model is critical. Ensemble learning offers a powerful way to achieve this. Here's a step-by-step approach for building and evaluating such a model using ensemble techniques.\n",
        "\n",
        "1. **Choose Between Bagging and Boosting**\n",
        "\n",
        "For loan default prediction, Boosting is the preferred choice over Bagging.\n",
        "\n",
        "Boosting excels at reducing bias and focusing on the most challenging cases. In a loan default scenario, correctly identifying high-risk individuals (who are often a minority class) is the most critical task. Boosting algorithms like Gradient Boosting or XGBoost sequentially train models to correct the errors of previous models, naturally putting more emphasis on the customers who were difficult to classify, thereby improving the predictive power on the default class.\n",
        "\n",
        "Bagging, while great for reducing variance and preventing overfitting, treats all data points equally. It doesn't specifically target the \"hard cases\" of potential defaulters, which are the most important for the business to identify.\n",
        "\n",
        "2. **Handle Overfitting**\n",
        "\n",
        "Boosting models can be prone to overfitting, so we'll use several strategies to mitigate this:\n",
        "\n",
        "Regularization Parameters: We'll limit the complexity of our individual base models (e.g., setting a max_depth for decision trees). We'll also use a learning_rate parameter to shrink the contribution of each new tree, which prevents the model from rapidly over-indexing on the training data.\n",
        "\n",
        "Early Stopping: We can monitor the model's performance on a validation set and stop training if the performance metric (e.g., log-loss) stops improving. This prevents the model from continuing to learn noise in the training data.\n",
        "\n",
        "Subsampling: We'll train each tree on a random subset of the training data and features, similar to Random Forest. This adds more randomness and helps to reduce variance.\n",
        "\n",
        "3. **Select Base Models**\n",
        "\n",
        "Decision trees are an excellent choice for base models in boosting ensembles.\n",
        "\n",
        "They are highly interpretable, which is crucial in a regulated industry like finance.\n",
        "\n",
        "They can handle both categorical (e.g., gender, marital status) and numerical (e.g., income, credit score) data without extensive preprocessing.\n",
        "\n",
        "When used as \"weak learners\" (i.e., shallow trees with limited max_depth), they are computationally efficient and form the perfect building blocks for a boosting algorithm.\n",
        "\n",
        "4. **Evaluate Performance Using Cross-Validation**\n",
        "\n",
        "We will use k-fold cross-validation to get a reliable estimate of the model's performance. Instead of a single train-test split, we'll divide the data into k folds, train the model k times, and average the performance metrics. This ensures our evaluation is not dependent on a specific data split.\n",
        "\n",
        "Performance Metrics: Given that loan defaults are a relatively rare event (class imbalance), simple accuracy is misleading. A model that predicts \"non-default\" for everyone might have a high accuracy but would be useless. We'll use more appropriate metrics like:\n",
        "\n",
        "AUC-ROC (Area Under the Receiver Operating Characteristic curve): Measures the model's ability to distinguish between the two classes.\n",
        "\n",
        "F1-Score: The harmonic mean of precision and recall, which is a better measure for imbalanced datasets.\n",
        "\n",
        "Precision and Recall: Precision is the proportion of predicted defaults that were actually defaults, while recall is the proportion of actual defaults that were correctly identified.\n",
        "\n",
        "5. **Justify How Ensemble Learning Improves Decision-Making**   \n",
        "\n",
        "Ensemble learning's strength lies in its ability to produce a more reliable and robust prediction than any single model. In the context of financial decision-making, this translates to:\n",
        "\n",
        "More Accurate Risk Assessment: By combining the insights of multiple models, the ensemble model can identify complex patterns that a single decision tree might miss. This leads to a more precise estimate of a customer's likelihood to default.\n",
        "\n",
        "Reduced Financial Loss: A more accurate model means fewer high-risk customers are incorrectly approved for loans, directly reducing the institution's financial losses from defaults.\n",
        "\n",
        "Fairer Decision-Making: A robust model is less likely to be swayed by random noise in the data, leading to more consistent and fairer decisions across different customer profiles.\n",
        "\n",
        "Python Code for the Approach\n",
        "The following code demonstrates a Boosting approach using GradientBoostingClassifier and evaluates it using GridSearchCV with cross-validation. We'll use a synthetic dataset for demonstration.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Python Code for the Approach\n",
        "The following code demonstrates a Boosting approach using GradientBoostingClassifier and evaluates it using GridSearchCV with cross-validation. We'll use a synthetic dataset for demonstration.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AfDpRd-VeGZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "# 1. Simulate a loan default dataset with class imbalance\n",
        "# We create a dataset with 10,000 samples and 20 features, with a 90/10 class split.\n",
        "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, n_classes=2, weights=[0.9, 0.1],\n",
        "                           flip_y=0.01, random_state=42)\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "y = pd.Series(y, name='default')\n",
        "\n",
        "print(\"Synthetic loan default dataset created. üìä\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of defaults (Class 1): {sum(y == 1)}\")\n",
        "print(f\"Number of non-defaults (Class 0): {sum(y == 0)}\\n\")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\\n\")\n",
        "\n",
        "# 2. Define the Boosting model and hyperparameters for tuning\n",
        "# We use GradientBoostingClassifier, a popular boosting algorithm.\n",
        "gbc = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "# We are tuning n_estimators, learning_rate, and max_depth to handle overfitting.\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees\n",
        "    'learning_rate': [0.05, 0.1, 0.2], # Contribution of each tree\n",
        "    'max_depth': [3, 5, 7]             # Max depth of each tree (weak learner)\n",
        "}\n",
        "\n",
        "print(\"Starting GridSearchCV for hyperparameter tuning... üõ†Ô∏è\\n\")\n",
        "\n",
        "# 3. Use GridSearchCV with cross-validation to find the best model\n",
        "# cv=5 means 5-fold cross-validation.\n",
        "# We use 'roc_auc' as the scoring metric due to class imbalance.\n",
        "grid_search = GridSearchCV(estimator=gbc, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nGridSearchCV complete! ‚ú®\\n\")\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_gbc_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best hyperparameters found: üéâ\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"- {param}: {value}\")\n",
        "\n",
        "# 4. Evaluate the best model on the test set\n",
        "# Get predictions and probabilities\n",
        "y_pred = best_gbc_model.predict(X_test)\n",
        "y_pred_proba = best_gbc_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"\\nFinal AUC-ROC score on the test set: {auc_roc:.4f}\\n\")\n",
        "\n",
        "print(\"Classification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v53kO_EMf4Hk",
        "outputId": "76075265-c4db-46b6-8bb3-bbaa306e39aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic loan default dataset created. üìä\n",
            "Number of samples: 10000\n",
            "Number of defaults (Class 1): 1034\n",
            "Number of non-defaults (Class 0): 8966\n",
            "\n",
            "Training set size: 8000\n",
            "Testing set size: 2000\n",
            "\n",
            "Starting GridSearchCV for hyperparameter tuning... üõ†Ô∏è\n",
            "\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
          ]
        }
      ]
    }
  ]
}